# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ibWSu8hJhr87sBaOzePSs0aW4teH8A-b
"""

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain.vectorstores import FAISS
from langchain_community.llms import HuggingFaceHub
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
import os
import tempfile  # Missing import for handling temporary files


# Load API key securely
openai_api_key = st.secrets["OPENAI_API_KEY"]
os.environ["OPENAI_API_KEY"] = openai_api_key

# Streamlit UI
st.title("ðŸ“„ Chat with Your Documents (RAG)")

# File uploader
uploaded_files = st.file_uploader("Upload Documents (PDF, TXT, DOCX)", type=["pdf", "txt", "docx"], accept_multiple_files=True)

doc_texts = []
if uploaded_files:
    for uploaded_file in uploaded_files:
        file_extension = uploaded_file.name.split(".")[-1]

        # Save uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as temp_file:
            temp_file.write(uploaded_file.read())
            temp_file_path = temp_file.name

        # Load document based on type
        if file_extension == "pdf":
            loader = PyPDFLoader(temp_file_path)
        elif file_extension == "txt":
            loader = TextLoader(temp_file_path)
        elif file_extension == "docx":
            loader = Docx2txtLoader(temp_file_path)
        else:
            st.error("Unsupported file format!")
            continue

        docs = loader.load()
        doc_texts.extend(docs)

    st.success("Documents loaded successfully!")

    # Convert to embeddings & create FAISS index
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(doc_texts, embeddings)
    retriever = vectorstore.as_retriever()

    # Set up RAG QA chain
    # Use a free LLM from Hugging Face (e.g., Mistral 7B)
    Llm = HuggingFaceHub(repo_id="mistralai/Mistral-7B-Instruct", model_kwargs={"temperature": 0.7})
    qa_chain = RetrievalQA.from_chain_type(llm=Llm, retriever=retriever)

    # Chat Interface
    user_query = st.text_input("Ask a question about the document:")
    if user_query:
        response = qa_chain.run(user_query)
        st.write("**Response:**", response)

st.markdown("---")
st.info("Powered by OpenAI & LangChain")
